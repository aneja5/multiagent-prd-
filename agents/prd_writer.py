"""PRDWriterAgent - synthesizes insights into PRD with citations.

This agent takes all collected evidence and analysis (pain points, competitors, gaps)
and synthesizes them into a comprehensive, citation-backed PRD using the template.
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from openai import OpenAI, OpenAIError
from pydantic import BaseModel, Field
from rich.console import Console

from agents.base_agent import BaseAgent
from app.logger import get_logger
from app.state import State, Task

logger = get_logger(__name__)
console = Console()


class RiskItem(BaseModel):
    """A risk with mitigation strategy."""

    risk: str = Field(description="Description of the risk")
    likelihood: str = Field(description="Likelihood: high, medium, or low")
    impact: str = Field(description="Impact: high, medium, or low")
    mitigation: str = Field(description="Mitigation strategy")


class PRDContent(BaseModel):
    """Complete PRD content generated by LLM."""

    product_name: str = Field(description="Product name (2-4 words)")
    problem_statement: str = Field(description="Clear problem statement (2-3 sentences)")
    target_users: str = Field(description="Detailed user description (2-3 sentences)")
    jtbd: str = Field(description="Jobs to Be Done (3-5 jobs)")
    current_workflow: str = Field(description="How users solve this today (1-2 paragraphs)")
    solution_overview: str = Field(description="High-level solution description (1-2 paragraphs)")
    value_proposition: str = Field(description="Clear value prop (1-2 sentences)")
    differentiators: List[str] = Field(description="3-5 key differentiators")
    mvp_features: List[str] = Field(description="5-8 MVP features")
    phase2_features: List[str] = Field(description="4-6 phase 2 features")
    future_features: List[str] = Field(description="3-5 future features")
    non_goals: List[str] = Field(description="3-5 explicit non-goals")
    user_workflows: str = Field(description="Key workflows (1-2 paragraphs)")
    data_integrations: str = Field(description="Data and integration needs (1 paragraph)")
    compliance_security: str = Field(description="Compliance and security requirements (1 paragraph)")
    success_metrics: List[str] = Field(description="5-7 key metrics")
    risks: List[RiskItem] = Field(description="4-6 risks with mitigation")
    rollout_plan: str = Field(description="High-level rollout approach (1-2 paragraphs)")


class PRDWriterAgent(BaseAgent):
    """Synthesize all insights into a comprehensive PRD.

    Process:
    1. Generate product name from metadata
    2. Use LLM to synthesize each PRD section
    3. Map citations (track which evidence supports which claims)
    4. Populate template with content
    5. Format tables (pain points, competitors, risks)
    6. Save to state.prd

    Attributes:
        name: Agent identifier ("prd_writer")
        client: OpenAI client for API calls
        template_path: Path to the PRD template file
    """

    def __init__(self, name: str, client: OpenAI) -> None:
        """Initialize the PRDWriterAgent.

        Args:
            name: Agent identifier (typically "prd_writer")
            client: Configured OpenAI client instance
        """
        super().__init__(name, client)
        self.template_path = Path("templates/prd_notion.md")
        self.logger = get_logger(__name__)

    def run(self, state: State) -> State:
        """Generate PRD from insights.

        Args:
            state: Current shared state containing insights and evidence

        Returns:
            Updated state with PRD added

        Raises:
            Exception: If PRD generation fails
        """
        self.logger.info("Starting PRD generation")
        self._log_action(state, "started_prd_generation")

        # Create task on task board
        task = Task(
            id=f"T-PRD-{state.run_id[:8]}",
            owner="prd_writer",
            status="doing",
            description="Generate PRD from research insights"
        )
        state.task_board.append(task)

        try:
            console.print("\n[bold cyan]Generating PRD...[/bold cyan]\n")

            # Validate we have enough data to generate PRD
            if not self._validate_inputs(state):
                self.logger.warning("Insufficient data for PRD generation")
                console.print("[yellow]Insufficient research data to generate PRD[/yellow]")
                self._mark_task_done(state, task.id)
                return state

            # Generate PRD content using LLM
            prd_content = self._generate_content(state)

            if not prd_content:
                self.logger.error("Failed to generate PRD content")
                console.print("[red]Failed to generate PRD content[/red]")
                self._mark_task_blocked(state, task.id)
                return state

            # Build citation map
            citation_map = self._build_citation_map(prd_content, state)

            self._log_action(
                state,
                "built_citation_map",
                details={"total_citations": sum(len(v) for v in citation_map.values())}
            )

            # Populate template
            prd_markdown = self._populate_template(prd_content, state, citation_map)

            # Save to state
            state.prd.sections = prd_content.model_dump()
            state.prd.notion_markdown = prd_markdown
            state.prd.citation_map = citation_map

            console.print(f"[green]✓ PRD generated ({len(prd_markdown):,} chars)[/green]")
            console.print(f"[green]✓ Product name: {prd_content.product_name}[/green]")
            console.print(f"[green]✓ MVP features: {len(prd_content.mvp_features)}[/green]")
            console.print(f"[green]✓ Risks identified: {len(prd_content.risks)}[/green]\n")

            # Update task board - mark prd_draft tasks as done
            for t in state.task_board:
                if t.owner == "prd_draft":
                    t.status = "done"

            self._mark_task_done(state, task.id)

            self._log_action(
                state,
                "completed_prd_generation",
                details={
                    "product_name": prd_content.product_name,
                    "prd_length": len(prd_markdown),
                    "mvp_features": len(prd_content.mvp_features),
                    "citations": sum(len(v) for v in citation_map.values())
                }
            )

            self.logger.info(f"PRD generation complete: {prd_content.product_name}")

            return state

        except Exception as e:
            self._mark_task_blocked(state, task.id)
            self.logger.error(f"PRD generation failed: {e}")
            self._log_action(state, f"prd_generation_failed: {str(e)}")
            raise

    def _validate_inputs(self, state: State) -> bool:
        """Validate that we have sufficient data for PRD generation.

        Args:
            state: Current state

        Returns:
            True if we have enough data, False otherwise
        """
        # Need at least some pain points or competitors
        has_pain_points = len(state.insights.pain_points) > 0
        has_competitors = len(state.insights.competitors) > 0

        if not has_pain_points and not has_competitors:
            self.logger.warning("No pain points or competitors found")
            return False

        # Need metadata to be set
        if not state.metadata.domain or not state.metadata.target_user:
            self.logger.warning("Missing domain or target user metadata")
            return False

        return True

    def _generate_content(self, state: State) -> Optional[PRDContent]:
        """Use LLM to generate PRD content.

        Args:
            state: Current state with insights

        Returns:
            PRDContent object or None on failure
        """
        # Load prompt template
        try:
            prompt_template = self._load_prompt()
        except FileNotFoundError:
            self.logger.error("PRD writer prompt file not found")
            raise

        # Prepare context
        context = self._prepare_context(state)

        # Build prompt with context
        prompt = prompt_template
        for key, value in context.items():
            prompt = prompt.replace(f"{{{{{key}}}}}", str(value))

        # Build messages for LLM
        messages = [
            {
                "role": "system",
                "content": (
                    "You are an expert product manager writing a Product Requirements Document. "
                    "Generate specific, actionable content grounded in the research provided. "
                    "Be concrete and avoid generic statements."
                )
            },
            {
                "role": "user",
                "content": prompt
            }
        ]

        # Build JSON schema for structured output
        json_schema = self._build_json_schema()

        self._log_action(
            state,
            "calling_llm_for_prd",
            details={
                "pain_points_count": len(state.insights.pain_points),
                "competitors_count": len(state.insights.competitors),
                "model": self.config.openai_model
            }
        )

        try:
            # Call LLM with structured output
            llm_response = self._call_llm(
                messages=messages,
                response_format={
                    "type": "json_schema",
                    "json_schema": json_schema
                },
                temperature=0.6,  # Moderate creativity for PRD content
                max_tokens=4000
            )

            # Parse response
            content = llm_response.get("content", "")
            if not content:
                self.logger.error("Empty response from LLM")
                return None

            # Parse JSON and validate with Pydantic
            data_dict = json.loads(content)
            result = PRDContent(**data_dict)

            self.logger.info(f"Generated PRD content for: {result.product_name}")

            return result

        except json.JSONDecodeError as e:
            self.logger.error(f"Failed to parse LLM response as JSON: {e}")
            return None
        except Exception as e:
            self.logger.error(f"Failed to generate PRD content: {e}")
            raise

    def _prepare_context(self, state: State) -> Dict[str, str]:
        """Prepare context for LLM prompt.

        Args:
            state: Current state

        Returns:
            Dictionary of context variables for prompt
        """
        # Format pain points
        pain_points_text = []
        for pp in state.insights.pain_points:
            # Handle both Pydantic models and dicts
            if hasattr(pp, 'model_dump'):
                pp_dict = pp.model_dump()
            else:
                pp_dict = pp if isinstance(pp, dict) else {}

            cluster_name = pp_dict.get('cluster_name', pp_dict.get('description', 'Unknown'))
            severity = pp_dict.get('severity', 'medium')
            what = pp_dict.get('what', pp_dict.get('description', ''))
            who = pp_dict.get('who', 'users')
            why = pp_dict.get('why', '')

            pain_points_text.append(
                f"- **{cluster_name}** ({severity}): "
                f"{what} (affects {who}). "
                f"Root cause: {why}"
            )

        # Format competitors
        competitors_text = []
        for comp in state.insights.competitors:
            if hasattr(comp, 'model_dump'):
                comp_dict = comp.model_dump()
            else:
                comp_dict = comp if isinstance(comp, dict) else {}

            name = comp_dict.get('name', 'Unknown')
            positioning = comp_dict.get('positioning', comp_dict.get('description', ''))
            key_features = comp_dict.get('key_features', [])
            weaknesses = comp_dict.get('weaknesses', [])

            features_str = ', '.join(key_features[:3]) if key_features else 'N/A'
            weaknesses_str = ', '.join(weaknesses[:2]) if weaknesses else 'N/A'

            competitors_text.append(
                f"- **{name}**: {positioning}. "
                f"Features: {features_str}. "
                f"Weaknesses: {weaknesses_str}"
            )

        # Format opportunity gaps
        gaps_text = "\n".join([f"- {gap}" for gap in state.insights.opportunity_gaps])

        # Format compliance contexts
        compliance = ", ".join(state.metadata.compliance_contexts) if state.metadata.compliance_contexts else "None specified"

        return {
            "domain": state.metadata.domain or "general",
            "target_user": state.metadata.target_user or "users",
            "geography": state.metadata.geography or "global",
            "compliance_contexts": compliance,
            "pain_points": "\n".join(pain_points_text) or "No pain points identified",
            "competitors": "\n".join(competitors_text) or "No competitors identified",
            "opportunity_gaps": gaps_text or "No opportunity gaps identified",
            "market_insights": state.insights.market_insights or "No market insights available",
        }

    def _build_json_schema(self) -> Dict[str, Any]:
        """Build JSON schema for OpenAI structured output.

        Returns:
            JSON schema dictionary compatible with OpenAI API
        """
        return {
            "name": "prd_content",
            "description": "Complete PRD content synthesized from research",
            "strict": True,
            "schema": {
                "type": "object",
                "properties": {
                    "product_name": {
                        "type": "string",
                        "description": "Product name (2-4 words)"
                    },
                    "problem_statement": {
                        "type": "string",
                        "description": "Clear problem statement (2-3 sentences)"
                    },
                    "target_users": {
                        "type": "string",
                        "description": "Detailed user description"
                    },
                    "jtbd": {
                        "type": "string",
                        "description": "Jobs to Be Done (3-5 jobs formatted as list)"
                    },
                    "current_workflow": {
                        "type": "string",
                        "description": "How users solve this today"
                    },
                    "solution_overview": {
                        "type": "string",
                        "description": "High-level solution description"
                    },
                    "value_proposition": {
                        "type": "string",
                        "description": "Clear value prop"
                    },
                    "differentiators": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "Key differentiators"
                    },
                    "mvp_features": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "MVP features"
                    },
                    "phase2_features": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "Phase 2 features"
                    },
                    "future_features": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "Future features"
                    },
                    "non_goals": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "Explicit non-goals"
                    },
                    "user_workflows": {
                        "type": "string",
                        "description": "Key user workflows"
                    },
                    "data_integrations": {
                        "type": "string",
                        "description": "Data and integration needs"
                    },
                    "compliance_security": {
                        "type": "string",
                        "description": "Compliance and security requirements"
                    },
                    "success_metrics": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "Key success metrics"
                    },
                    "risks": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "risk": {"type": "string"},
                                "likelihood": {"type": "string"},
                                "impact": {"type": "string"},
                                "mitigation": {"type": "string"}
                            },
                            "required": ["risk", "likelihood", "impact", "mitigation"],
                            "additionalProperties": False
                        },
                        "description": "Risks with mitigation"
                    },
                    "rollout_plan": {
                        "type": "string",
                        "description": "Rollout approach"
                    }
                },
                "required": [
                    "product_name", "problem_statement", "target_users", "jtbd",
                    "current_workflow", "solution_overview", "value_proposition",
                    "differentiators", "mvp_features", "phase2_features",
                    "future_features", "non_goals", "user_workflows",
                    "data_integrations", "compliance_security", "success_metrics",
                    "risks", "rollout_plan"
                ],
                "additionalProperties": False
            }
        }

    def _build_citation_map(self, prd_content: PRDContent, state: State) -> Dict[str, List[str]]:
        """Map PRD sections to evidence IDs.

        Args:
            prd_content: Generated PRD content
            state: Current state with evidence

        Returns:
            Dictionary mapping section names to evidence IDs
        """
        citation_map: Dict[str, List[str]] = {
            "problem_statement": [],
            "pain_points": [],
            "competitors": [],
            "mvp_features": [],
            "solution_overview": [],
        }

        # Link pain points to their evidence
        for pp in state.insights.pain_points:
            if hasattr(pp, 'evidence_ids'):
                evidence_ids = pp.evidence_ids
            elif isinstance(pp, dict):
                evidence_ids = pp.get("evidence_ids", [])
            else:
                evidence_ids = []
            citation_map["pain_points"].extend(evidence_ids)

        # Link competitors to their evidence
        for comp in state.insights.competitors:
            if hasattr(comp, 'evidence_ids'):
                evidence_ids = comp.evidence_ids
            elif isinstance(comp, dict):
                evidence_ids = comp.get("evidence_ids", [])
            else:
                evidence_ids = []
            citation_map["competitors"].extend(evidence_ids)

        # Problem statement citations come from pain points
        citation_map["problem_statement"] = citation_map["pain_points"][:5]

        # Solution overview citations from both
        citation_map["solution_overview"] = (
            citation_map["pain_points"][:3] +
            citation_map["competitors"][:3]
        )

        # MVP features link to pain point evidence
        citation_map["mvp_features"] = citation_map["pain_points"][:10]

        # Deduplicate all citation lists
        for key in citation_map:
            citation_map[key] = list(set(citation_map[key]))

        return citation_map

    def _populate_template(
        self,
        content: PRDContent,
        state: State,
        citation_map: Dict[str, List[str]]
    ) -> str:
        """Populate template with content.

        Args:
            content: PRD content from LLM
            state: Current state
            citation_map: Map of sections to evidence IDs

        Returns:
            Populated PRD markdown
        """
        # Load template
        try:
            with open(self.template_path, "r", encoding="utf-8") as f:
                template = f.read()
        except FileNotFoundError:
            self.logger.error(f"Template not found: {self.template_path}")
            raise

        # Build pain points table
        pain_points_table = self._build_pain_points_table(state)

        # Build competitors table
        competitors_table = self._build_competitors_table(state)

        # Build risks table
        risks_table = self._build_risks_table(content.risks)

        # Build evidence sections
        pain_points_evidence = self._build_evidence_section(
            state.insights.pain_points,
            state.evidence,
            "pain_points"
        )

        competitors_evidence = self._build_evidence_section(
            state.insights.competitors,
            state.evidence,
            "competitors"
        )

        key_sources = self._build_key_sources(state.evidence, citation_map)

        # Count evidence by type
        evidence_counts = {"forum": 0, "review": 0, "pricing": 0, "article": 0, "docs": 0}
        for e in state.evidence:
            if hasattr(e, 'type'):
                etype = e.type
            elif isinstance(e, dict):
                etype = e.get("type", "article")
            else:
                etype = "article"
            if etype in evidence_counts:
                evidence_counts[etype] += 1

        # Format opportunity gaps
        opportunity_gaps = state.insights.opportunity_gaps
        if opportunity_gaps:
            opportunity_gaps_str = "\n".join([f"- {gap}" for gap in opportunity_gaps])
        else:
            opportunity_gaps_str = "*(No specific opportunity gaps identified)*"

        # Build replacements dictionary
        replacements = {
            "{{product_name}}": content.product_name,
            "{{owner}}": "Product Team",
            "{{date}}": datetime.now().strftime("%Y-%m-%d"),
            "{{evidence_count}}": str(len(state.evidence)),
            "{{run_id}}": state.run_id,

            "{{problem_statement}}": self._add_citations(
                content.problem_statement,
                citation_map.get("problem_statement", [])
            ),
            "{{target_users}}": content.target_users,
            "{{jtbd}}": content.jtbd,
            "{{current_workflow}}": content.current_workflow,

            "{{pain_points_table}}": pain_points_table,
            "{{competitors_table}}": competitors_table,
            "{{opportunity_gaps}}": opportunity_gaps_str,

            "{{solution_overview}}": self._add_citations(
                content.solution_overview,
                citation_map.get("solution_overview", [])
            ),
            "{{value_proposition}}": content.value_proposition,
            "{{differentiators}}": "\n".join([f"- {d}" for d in content.differentiators]),

            "{{mvp_features}}": "\n".join([f"- {f}" for f in content.mvp_features]),
            "{{phase2_features}}": "\n".join([f"- {f}" for f in content.phase2_features]),
            "{{future_features}}": "\n".join([f"- {f}" for f in content.future_features]),
            "{{non_goals}}": "\n".join([f"- {ng}" for ng in content.non_goals]),

            "{{user_workflows}}": content.user_workflows,
            "{{user_journeys}}": "*(To be defined in detailed design phase)*",

            "{{data_integrations}}": content.data_integrations,
            "{{compliance_security}}": content.compliance_security,

            "{{success_metrics}}": "\n".join([f"- {m}" for m in content.success_metrics]),
            "{{success_criteria}}": "*(To be defined with stakeholders)*",

            "{{risks_table}}": risks_table,
            "{{rollout_plan}}": content.rollout_plan,

            "{{forum_count}}": str(evidence_counts["forum"]),
            "{{review_count}}": str(evidence_counts["review"]),
            "{{pricing_count}}": str(evidence_counts["pricing"]),
            "{{article_count}}": str(evidence_counts["article"]),

            "{{pain_points_evidence}}": pain_points_evidence,
            "{{competitors_evidence}}": competitors_evidence,
            "{{key_sources}}": key_sources,
        }

        result = template
        for placeholder, value in replacements.items():
            result = result.replace(placeholder, value)

        return result

    def _build_pain_points_table(self, state: State) -> str:
        """Build pain points table in markdown.

        Args:
            state: Current state with pain points

        Returns:
            Markdown table string
        """
        if not state.insights.pain_points:
            return "*(No pain points identified)*"

        table = "| Pain Point | Who | What | Severity | Evidence |\n"
        table += "|------------|-----|------|----------|----------|\n"

        for pp in state.insights.pain_points:
            if hasattr(pp, 'model_dump'):
                pp_dict = pp.model_dump()
            else:
                pp_dict = pp if isinstance(pp, dict) else {}

            cluster_name = pp_dict.get('cluster_name', pp_dict.get('description', 'Unknown'))[:30]
            who = pp_dict.get('who', 'users')[:40]
            what = pp_dict.get('what', pp_dict.get('description', ''))[:50]
            severity = pp_dict.get('severity', 'medium')
            evidence_ids = pp_dict.get('evidence_ids', [])

            # Truncate with ellipsis if needed
            who_display = f"{who}..." if len(pp_dict.get('who', '')) > 40 else who
            what_display = f"{what}..." if len(pp_dict.get('what', '')) > 50 else what

            evidence_links = ", ".join([f"`{eid}`" for eid in evidence_ids[:3]])

            table += f"| **{cluster_name}** | {who_display} | {what_display} | {severity} | {evidence_links} |\n"

        return table

    def _build_competitors_table(self, state: State) -> str:
        """Build competitors table in markdown.

        Args:
            state: Current state with competitors

        Returns:
            Markdown table string
        """
        if not state.insights.competitors:
            return "*(No competitors identified)*"

        table = "| Competitor | Positioning | Key Features | Strengths | Weaknesses |\n"
        table += "|------------|-------------|--------------|-----------|------------|\n"

        for comp in state.insights.competitors:
            if hasattr(comp, 'model_dump'):
                comp_dict = comp.model_dump()
            else:
                comp_dict = comp if isinstance(comp, dict) else {}

            name = comp_dict.get('name', 'Unknown')
            positioning = comp_dict.get('positioning', comp_dict.get('description', ''))[:40]
            key_features = comp_dict.get('key_features', [])
            strengths = comp_dict.get('strengths', [])
            weaknesses = comp_dict.get('weaknesses', [])

            # Truncate with ellipsis
            positioning_display = f"{positioning}..." if len(comp_dict.get('positioning', '')) > 40 else positioning

            features_str = ", ".join(key_features[:3]) + "..." if key_features else "N/A"
            strengths_str = ", ".join(strengths[:2]) if strengths else "N/A"
            weaknesses_str = ", ".join(weaknesses[:2]) if weaknesses else "N/A"

            table += f"| **{name}** | {positioning_display} | {features_str} | {strengths_str} | {weaknesses_str} |\n"

        return table

    def _build_risks_table(self, risks: List[RiskItem]) -> str:
        """Build risks table in markdown.

        Args:
            risks: List of RiskItem objects

        Returns:
            Markdown table string
        """
        if not risks:
            return "*(No risks identified)*"

        table = "| Risk | Likelihood | Impact | Mitigation |\n"
        table += "|------|------------|--------|------------|\n"

        for risk in risks:
            if hasattr(risk, 'model_dump'):
                risk_dict = risk.model_dump()
            else:
                risk_dict = risk if isinstance(risk, dict) else {}

            risk_desc = risk_dict.get('risk', 'Unknown risk')
            likelihood = risk_dict.get('likelihood', 'medium')
            impact = risk_dict.get('impact', 'medium')
            mitigation = risk_dict.get('mitigation', 'N/A')

            table += f"| {risk_desc} | {likelihood} | {impact} | {mitigation} |\n"

        return table

    def _build_evidence_section(
        self,
        insights: List[Any],
        evidence: List[Any],
        section: str
    ) -> str:
        """Build evidence section for insights.

        Args:
            insights: List of insight objects (pain points or competitors)
            evidence: List of evidence objects
            section: Section name (for logging)

        Returns:
            Markdown string of evidence
        """
        if not insights:
            return "*(No evidence available)*"

        output = []

        for insight in insights[:5]:  # Top 5
            if hasattr(insight, 'model_dump'):
                insight_dict = insight.model_dump()
            else:
                insight_dict = insight if isinstance(insight, dict) else {}

            name = insight_dict.get("cluster_name") or insight_dict.get("name", "Unknown")
            evidence_ids = insight_dict.get("evidence_ids", [])

            output.append(f"\n**{name}**")

            for eid in evidence_ids[:3]:  # Top 3 sources
                # Find evidence
                ev = None
                for e in evidence:
                    if hasattr(e, 'id'):
                        if e.id == eid:
                            ev = e.model_dump() if hasattr(e, 'model_dump') else e
                            break
                    elif isinstance(e, dict) and e.get("id") == eid:
                        ev = e
                        break

                if ev:
                    title = ev.get('title', 'Untitled')[:60]
                    url = ev.get('url', '#')
                    etype = ev.get('type', 'source')
                    credibility = ev.get('credibility', 'unknown')
                    output.append(f"- [{title}]({url}) ({etype}, {credibility} credibility)")

        return "\n".join(output) if output else "*(No evidence linked)*"

    def _build_key_sources(
        self,
        evidence: List[Any],
        citation_map: Dict[str, List[str]]
    ) -> str:
        """Build key sources list.

        Args:
            evidence: List of evidence objects
            citation_map: Map of sections to evidence IDs

        Returns:
            Markdown string of key sources
        """
        if not evidence:
            return "*(No sources available)*"

        # Get all citations
        all_citations = []
        for citations in citation_map.values():
            all_citations.extend(citations)

        # Count citation frequency
        citation_counts: Dict[str, int] = {}
        for eid in all_citations:
            citation_counts[eid] = citation_counts.get(eid, 0) + 1

        # Sort by citation count
        top_sources = sorted(citation_counts.items(), key=lambda x: x[1], reverse=True)[:10]

        output = []
        for eid, count in top_sources:
            # Find evidence
            ev = None
            for e in evidence:
                if hasattr(e, 'id'):
                    if e.id == eid:
                        ev = e.model_dump() if hasattr(e, 'model_dump') else e
                        break
                elif isinstance(e, dict) and e.get("id") == eid:
                    ev = e
                    break

            if ev:
                title = ev.get('title', 'Untitled')[:60]
                url = ev.get('url', '#')
                etype = ev.get('type', 'source')
                output.append(f"- `{eid}` [{title}]({url}) ({etype}, cited {count}x)")

        # If no top sources, just list first 10 evidence items
        if not output:
            for e in evidence[:10]:
                if hasattr(e, 'model_dump'):
                    ev = e.model_dump()
                else:
                    ev = e if isinstance(e, dict) else {}

                eid = ev.get('id', 'unknown')
                title = ev.get('title', 'Untitled')[:60]
                url = ev.get('url', '#')
                etype = ev.get('type', 'source')
                output.append(f"- `{eid}` [{title}]({url}) ({etype})")

        return "\n".join(output) if output else "*(No key sources identified)*"

    def _add_citations(self, text: str, evidence_ids: List[str]) -> str:
        """Add citation markers to text.

        Args:
            text: Original text
            evidence_ids: Evidence IDs to cite

        Returns:
            Text with citation markers
        """
        if evidence_ids:
            citations = ",".join(evidence_ids[:3])
            return f"{text} `[evidence:{citations}]`"
        return text

    def _mark_task_done(self, state: State, task_id: str) -> None:
        """Mark a task as done on the task board.

        Args:
            state: Current state
            task_id: ID of task to mark done
        """
        for t in state.task_board:
            if t.id == task_id:
                t.status = "done"
                break

    def _mark_task_blocked(self, state: State, task_id: str) -> None:
        """Mark a task as blocked on the task board.

        Args:
            state: Current state
            task_id: ID of task to mark blocked
        """
        for t in state.task_board:
            if t.id == task_id:
                t.status = "blocked"
                break
